# nlp-readings


### Language Models

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <a href="https://arxiv.org/pdf/1810.04805.pdf">Link</a> \
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators <a href="https://arxiv.org/pdf/2003.10555.pdf ">Link</a> \
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations <a href="https://arxiv.org/pdf/1909.11942.pdf">Link</a> \ 


### Question Answering

Reading Comprehension Based Question Answering - SQuAD (https://rajpurkar.github.io/SQuAD-explorer) \
Conversational Question Answering - CoQA (https://stanfordnlp.github.io/coqa) \
Reading Wikipedia to Answer Open Domain Questions (https://arxiv.org/abs/1704.00051) ACL - 2017 \
Adversarial Examples for Evaluating Reading Comprehension Systems (SQuAD) https://arxiv.org/pdf/1707.07328.pdf \
A Detailed Account of The First Question Generation Shared Task https://www.aclweb.org/anthology/W10-4234.pdf \
Extending Neural Question Answering with Linguistic Input Features https://www.aclweb.org/anthology/W19-5806.pdf \
Knowledge Graph Embedding Based Question Answering http://research.baidu.com/Public/uploads/5c1c9a58317b3.pdf \ 
BERT+vnKG: Using Deep Learning and Knowledge Graph to Improve Vietnamese Question Answering System \ https://www.researchgate.net/publication/343423827_BERTvnKG_Using_Deep_Learning_and_Knowledge_Graph_to_Improve_Vietnamese_Question_Answering_System


