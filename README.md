# nlp-readings


### Language Models

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://arxiv.org/pdf/1810.04805.pdf \
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators https://arxiv.org/pdf/2003.10555.pdf \
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations https://arxiv.org/pdf/1909.11942.pdf \


### Question Answering

Reading Comprehension Based Question Answering - SQuAD (https://rajpurkar.github.io/SQuAD-explorer) \
Conversational Question Answering - CoQA (https://stanfordnlp.github.io/coqa) \
Reading Wikipedia to Answer Open Domain Questions (https://arxiv.org/abs/1704.00051) ACL - 2017 \
Adversarial Examples for Evaluating Reading Comprehension Systems (SQuAD)h https://arxiv.org/pdf/1707.07328.pdf
